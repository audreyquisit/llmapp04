name: DeepEval Tests

on:
  push:
    branches: [main]
    paths:
      - 'deepeval-tests/**'
  pull_request:
    branches: [main]
    paths:
      - 'deepeval-tests/**'
  workflow_dispatch:
    inputs:
      api_base_url:
        description: 'Base URL of the API to test'
        required: false
        default: 'http://localhost:8080'

env:
  API_BASE_URL: ${{ github.event.inputs.api_base_url || 'http://localhost:8080' }}
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

jobs:
  deepeval-tests:
    runs-on: ubuntu-latest
    
    services:
      llm-python:
        image: ${{ secrets.DOCKERHUB_USERNAME }}/llm-python:latest
        ports:
          - 8080:8080
        env:
          OLLAMA_BASE_URL: ${{ secrets.OLLAMA_BASE_URL }}
          OLLAMA_MODEL: ${{ secrets.OLLAMA_MODEL }}
          OLLAMA_API_KEY: ${{ secrets.OLLAMA_API_KEY }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        working-directory: ./deepeval-tests
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Wait for API to be ready
        run: |
          echo "Waiting for API to be ready..."
          for i in {1..30}; do
            if curl -s http://localhost:8080/swagger-ui.html > /dev/null 2>&1; then
              echo "API is ready!"
              break
            fi
            echo "Attempt $i: API not ready yet, waiting..."
            sleep 5
          done

      - name: Run DeepEval classify tests
        working-directory: ./deepeval-tests
        run: deepeval test run test_classify.py -v
        continue-on-error: true

      - name: Run DeepEval sentiment tests
        working-directory: ./deepeval-tests
        run: deepeval test run test_sentiment.py -v
        continue-on-error: true

      - name: Run DeepEval summarize tests
        working-directory: ./deepeval-tests
        run: deepeval test run test_summarize.py -v
        continue-on-error: true

      - name: Run DeepEval intent tests
        working-directory: ./deepeval-tests
        run: deepeval test run test_intent.py -v
        continue-on-error: true

